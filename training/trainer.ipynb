{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "together-mistake",
   "metadata": {},
   "source": [
    "### Importing Packages\n",
    "\n",
    "All imported packages are here, so we don't need to search for them everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import cloudpickle\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-auditor",
   "metadata": {},
   "source": [
    "### Data Extraction\n",
    "\n",
    "We'll load the dataset from the environment variable `DATASET_PATH` but only the columns we are interested in, as a good practice. It doesn't make that much difference in this case, but if our dataset was huge it could potentially save us a lot of memory.\n",
    "\n",
    "For this exercise, we'll use only the text columns `title` and `concatenated_tags`, since during an early experimentation phase, these two columns were enough for a very good result, and although working with additional numerical features like `price` and `weight` could help improve the model even further, that would require a lot more tricks with sparse matrix convertions and would be more complicated that what the exercise requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intended-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    os.environ[\"DATASET_PATH\"],\n",
    "    usecols=[\"title\", \"concatenated_tags\", \"category\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-leadership",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "\n",
    "Here we start by dropping any rows with NA values, because there are only two of them (about 0.05%). Then, we proceed to split the dataset into training and test sets, setting a random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "received-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sufficient-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[[\"title\", \"concatenated_tags\"]], df[\"category\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-reviewer",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "We are going to vectorize both our text columns (independently) and use them to fit a `MultinomialNB` classifier which is usually good at handling discrete features such as word counts for text classification.\n",
    "\n",
    "It's very important that when we transform the validation sets inside cross-validation or the test set after training, that we do so with the vectorizers fitted to the training data. To handle that and also to streamline the process and help with reproducibility, we will use a `Pipeline` with a custom transformer `TitleTagsVectorizer`.\n",
    "\n",
    "\n",
    "Note: The multinomial naive bayes model was chosen because, during an early experimentation phase, it showed good results with fast training times, which allows us to use more parameters in grid search without it taking too much time for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suffering-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitleTagsVectorizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to vectorize `title` and `concatenated_tags` columns\"\"\"\n",
    "    \n",
    "    def __init__(self, title_ngram_range=(1, 1), tags_ngram_range=(1, 1)):\n",
    "        self.title_ngram_range = title_ngram_range\n",
    "        self.tags_ngram_range = tags_ngram_range\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.title_cv = CountVectorizer(ngram_range=self.title_ngram_range)\n",
    "        self.title_cv.fit(X[\"title\"])\n",
    "        \n",
    "        self.tags_cv = CountVectorizer(ngram_range=self.tags_ngram_range)\n",
    "        self.tags_cv.fit(X[\"concatenated_tags\"])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        title_vect = self.title_cv.transform(X[\"title\"])\n",
    "        tags_vect = self.tags_cv.transform(X[\"concatenated_tags\"])\n",
    "        return sp.hstack([title_vect, tags_vect])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "coral-third",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(TitleTagsVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-embassy",
   "metadata": {},
   "source": [
    "### Model Validation\n",
    "\n",
    "For hyperparameter optimization and validation we will use scikit-learn's `GridSearchCV`, that performs a cross-validated grid-search over the parameter space. By default it uses a 5-fold CV.\n",
    "\n",
    "Note: The parameter grid we are using here might not actually contain the best values, but it serves a good demo purpose and achieves a nice result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "monetary-chain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "              Bebê       0.93      0.91      0.92      1780\n",
      "Bijuterias e Jóias       0.97      0.95      0.96       229\n",
      "         Decoração       0.91      0.92      0.91      2145\n",
      "     Lembrancinhas       0.93      0.94      0.94      4381\n",
      "            Outros       0.83      0.72      0.77       280\n",
      "       Papel e Cia       0.83      0.81      0.82       685\n",
      "\n",
      "          accuracy                           0.91      9500\n",
      "         macro avg       0.90      0.88      0.89      9500\n",
      "      weighted avg       0.91      0.91      0.91      9500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"titletagsvectorizer__title_ngram_range\": [(1, 2), (1, 3)],\n",
    "    \"titletagsvectorizer__tags_ngram_range\": [(1, 2), (1, 3)],\n",
    "    \"multinomialnb__alpha\": [1.0e-5, 1.0e-2, 1]\n",
    "}\n",
    "\n",
    "classifier = GridSearchCV(pipe, param_grid=param_grid, n_jobs=cpu_count() - 1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "report = classification_report(y_test, classifier.predict(X_test))\n",
    "print(report)\n",
    "\n",
    "with open(os.environ[\"METRICS_PATH\"], \"w\") as metrics_file:\n",
    "    metrics_file.write(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-smooth",
   "metadata": {},
   "source": [
    "### Model exportation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-grant",
   "metadata": {},
   "source": [
    "Here I use cloudpickle instead of pickle or joblib because they didn't handle well the serialization of the custom transformer as well as cloudpickle does, saving everything into a single .pkl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increasing-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.environ[\"MODEL_PATH\"], \"wb\") as model_file:\n",
    "    cloudpickle.dump(classifier, model_file)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
